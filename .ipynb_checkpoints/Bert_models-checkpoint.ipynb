{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af78b4ab",
   "metadata": {},
   "source": [
    "#Â Machine learning models for Name-Entity-Recgnition\n",
    "Summary of the paper [https://arxiv.org/pdf/1812.09449.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164562a2",
   "metadata": {},
   "source": [
    "The goal is to summarize machine learning algorithms used so far to tackle N.E.R tasks. The corresponding models vary from earlier ones to state-of-art models such as Generative adversarial Network, Transformers, Reinforcement learning etc...  . Then, we propose an implementation of the models and compare their performance on a N.E.R dataset taken from \"kaggle\". \n",
    "\n",
    "The idea behind this undeavor is to go in depth on the architecture of N.E.R models, so that we will be able to make educative decision on to what model to choose given a particular dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e3ac9",
   "metadata": {},
   "source": [
    "### What is Name Entity Recognition? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ca051",
   "metadata": {},
   "source": [
    "Name-Entity-Recognition(N.E.R) consists in associating entities(semantic types) to words in sentence. The entities could be: person, organisation, location, etc...  N.E.R plays an essential role in a variety of natural language processing applications such as information retrieval, automatic text summarization, question answering, machine translation, knowledge base contruction etc... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed6210",
   "metadata": {},
   "source": [
    "### N.E.R techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365012a8",
   "metadata": {},
   "source": [
    "Name entities are generally classfied in two categories : `generic NEs` (eg: person and location) and `domain-specific NEs` (e.g: protein, enzymes, and genes). The focus here will be on generic english based NEs.\n",
    "\n",
    "As to the `techniques` applied in NER, there are four main streams: \n",
    "\n",
    "- Rule-based approaches : Which do not need annotated data as they rely on hand-crafted rules;\n",
    "\n",
    "- Unsupervised learning approches : which rely on unsupervised learning approches without hand-labeled training samples;\n",
    "\n",
    "- Feature-based supervised learning approches: which rely on supervised learning algorithms ;\n",
    "\n",
    "- Deep-learning based approaches: which automatically discover representations needed for the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf96d2",
   "metadata": {},
   "source": [
    "### The Structure of the Paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e10684",
   "metadata": {},
   "source": [
    "- 1. Background:  definition , resources , evaluation metrics, and traditional approaches of NER;\n",
    "\n",
    "- 2. Deep learning techniques for NER; \n",
    "\n",
    "- 3. Summarizes recent applied deep learning techniques; \n",
    "\n",
    "- 4. Implementation of the models;\n",
    "\n",
    "- 5. Challenges and misconceptions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576df88",
   "metadata": {},
   "source": [
    "### 1 - Background\n",
    "\n",
    "Before delving into the how deep learning is used in NER field, we first explain the NER concept. We then introduce the widely used `NER datasets and tools`. Next, we detail the evaluation metrics and summarize the traditional approaches to NER .  \n",
    "\n",
    "An illustration of the `Named-entity-Recognition task`:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/N.E.R\" height=40, width=400> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861bde8",
   "metadata": {},
   "source": [
    "#### 1.1 N.E.R \n",
    "See above for more detail . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296c64d",
   "metadata": {},
   "source": [
    "#### 1.2 NER Resources :Datasets and tools \n",
    "\n",
    "Here is presented the widely used datasets and off-the-shelf tools for English NER.\n",
    "\n",
    "In table1 are presented some of the annotated corpus used for training NER models.  \n",
    "\n",
    "<img src=\"images/corpus\" height=50, width=600> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e73fad",
   "metadata": {},
   "source": [
    "In table2 are presented some of the tools used to annotate corpus . \n",
    "\n",
    "<img src=\"images/tools\" height=50, width=300> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37dde5",
   "metadata": {},
   "source": [
    "#### 1.3 NER Evaluation Metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffb7d6",
   "metadata": {},
   "source": [
    "NER models are usually evaluated by comparing their outputs against human annotations. The comparison can quantify either as `exact-match` or `relaxed match`. \n",
    "\n",
    "##### 1.3.1-Exact-match Evaluation\n",
    "\n",
    "NER involves identifying both entity boundaries and entity types. In `Exact math Evaluation`, a name entity is considered to be correctly recognized if both its boundary and type match ground truth. The performance metrics `Precision, Recall , F-score` are computed on the number of `True positives(TP), False Positive(FP), False Negatives (FN)`. \n",
    "\n",
    "- `TP` : entities recognised by the NER and match the ground truth.  \n",
    "- `FP` : entities recgonised by the NER but not match the ground truth \n",
    "- `FN` : entities not recgonised by the NER but annotated in the ground truth .  \n",
    "\n",
    "\n",
    "**Precision** = Among the outputs recognized as entities what is the proportion matching the ground truth. It measures the ability to present correct entities  = $\\frac{TP}{TP + FP}$\n",
    "\n",
    "**Recall** = measures the ability to recognize all entities in the corpus $\\frac{TP}{TP+FN}$\n",
    "\n",
    "**F-score** = harmonic mean between Recall and Precision= $2\\times{\\frac{Precision\\times Recall}{Precision + Recall}}$\n",
    "\n",
    "\n",
    "##### 1.3.2- Relaxed-match Evaluation \n",
    "\n",
    "A well known definition of `Relaxed-match evaluation` is that it consideres an entity as true if its type matches with that of the ground truth regarless of the boundaries as long as the latter overlaps with the ground truth boundaries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd9891",
   "metadata": {},
   "source": [
    "#### 1-4 Traditional Approaches to NER \n",
    "\n",
    "Traditional approaches to NER include : `rule-based approaches`, `unsupervised learning approches`, and `feature-based supervised learning approaches`. \n",
    "\n",
    "- **Rule-based approaches** : \n",
    "They are mainly hand-crafted based approaches. Some examples include: LaSIE, NetOwl, Facile, SAR, FASTUS, and LTG systems. The Rule based systems work very well when lexicon is exhaustive. With incomplete dictionaries, it leads to high precision and low recall . \n",
    "\n",
    "- **Unsupervised Learning Approaches** : \n",
    "A typical approach of unsupervised learning is clustering-based NER. The key idea is that `lexical patterns, lexical ressources and statistics` computed on a large corpus can be used to infer mentions of named entities. \n",
    "\n",
    "\n",
    "- **Feature-based Supervised Learning Approaches**: \n",
    "Given annotated data sample, features are designed to represent each training example.  Machine learing algorithms are then developed to recognize similar pattern from unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97354b",
   "metadata": {},
   "source": [
    "### 2-Deep learning techniques for NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca6b1f",
   "metadata": {},
   "source": [
    "#### 2-1 Why Deep learning for NER? \n",
    "Compared to feature based approaches, deep learning is beneficial in discovering hidden features automatically.Its key advantage is its capacity of representation learning and semantic composition through both vector representation and neural processing. There are three core strengths of applying deep learning techniques to NER. `First`, NER benefits from the `non-linear transformation`, which generates non-linear mapping from inputs to outputs. Compared to liner models (e.g: log-linear HMM and linear chain CRF), deep learning models are able to learn complexe and intricate features from data via non-linear activation functions. `Second`, Deeplearning saves significant efforts on designing NER features, contrary to feature based models which requiere considerable amount of engineering skills and domaine expertise. `Third`, thanks to gradient descent, complexe deep learning NER based models can be built. \n",
    "\n",
    "**The Taxonomy of DL-based NER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6e0a",
   "metadata": {},
   "source": [
    "<img src=\"Images/taxonomy.png\"  width=300, height=40 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c52648",
   "metadata": {},
   "source": [
    "- `Distributed representation for input`: \n",
    "The distributed representation for input consider word- and character-level embeddings as well as incorporatation of additional features like POS tag and \"gazeteer\" that have been effective in feature-based approaches.\n",
    "\n",
    "- `Context encoder`:\n",
    "Context encoder is used to capture the context dependencies between tokens of the input sequence using CNN, RNN, etc...  \n",
    "\n",
    "- `tag decoder `:\n",
    "Tag decoder predicts tag of tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd432d9",
   "metadata": {},
   "source": [
    "#### 2-2 Distributed Representations for Input \n",
    "\n",
    "Contrary to one-hot-encoding representation of words which represents words by sparse vectors , the distributed representations is a low dimentional, real-value, dense representation of words.  Ditributed representation capture from the text , the semantic and the syntatic properties of words. There are `three types of distributed representations` used in NER tasks: `word level`, `character level`, `hybrid representation`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9220f246",
   "metadata": {},
   "source": [
    "###### `2.2.1 Word-level Representation` \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c94ceb",
   "metadata": {},
   "source": [
    "###### `2.2.2 Character-level Representation` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5bae9a",
   "metadata": {},
   "source": [
    "###### `2.2.3 hybrid Representation`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e33cc",
   "metadata": {},
   "source": [
    "#### 2-3 Context Encoder Architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa6343",
   "metadata": {},
   "source": [
    "###### `2.3.1 Convolutional Neural Networks `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ab5f0",
   "metadata": {},
   "source": [
    "###### `2.3.2 Recurrent Neural Networks `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfa922",
   "metadata": {},
   "source": [
    "###### `2.3.3 Recursive Neural Networks `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11577f6",
   "metadata": {},
   "source": [
    "###### `2.3.4 Neural Language Models `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd49f9",
   "metadata": {},
   "source": [
    "###### 2.3.5 Deep Transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e427ce",
   "metadata": {},
   "source": [
    "#### 2.4 Tag Decoder Architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbda0ab",
   "metadata": {},
   "source": [
    "###### `2.4.1 Multi-layer Perceptron + softmax `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd9ecf9",
   "metadata": {},
   "source": [
    "###### `2.4.2 Conditional Random Fields `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a05b79",
   "metadata": {},
   "source": [
    "###### `2.4.3 Recurrent Neural Networks` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f54264",
   "metadata": {},
   "source": [
    "###### `2.4.4 Pointer Networks `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47078b8a",
   "metadata": {},
   "source": [
    "#### 2.5 Summary of DL-based NER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0fb88",
   "metadata": {},
   "source": [
    "### 3-Recent Deep -Learning Models for NER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd54fc1",
   "metadata": {},
   "source": [
    "##### `3-1 Deep Multi-task learning for NER `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11a307",
   "metadata": {},
   "source": [
    "##### `3-1 Deep Transfer learning for NER `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e256f9",
   "metadata": {},
   "source": [
    "##### `3-2 Deep Active learning for NER` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b44650",
   "metadata": {},
   "source": [
    "##### `3.3 Deep Reinforcement learning for NER `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41c469",
   "metadata": {},
   "source": [
    "##### `3.4 Deep Adversarial learning for NER `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f75260",
   "metadata": {},
   "source": [
    "##### `3.5 Neural Attention for NER` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9661ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
